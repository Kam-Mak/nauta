

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>OpenVINO Model Server Overview &mdash; Nauta 1.1 documentation</title>
  

  
  
    <link rel="shortcut icon" href="../../_static/favicon.png"/>
  
  
  

  
  <script type="text/javascript" src="../../_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
        <script type="text/javascript" src="../../_static/jquery.js"></script>
        <script type="text/javascript" src="../../_static/underscore.js"></script>
        <script type="text/javascript" src="../../_static/doctools.js"></script>
        <script type="text/javascript" src="../../_static/language_data.js"></script>
    
    <script type="text/javascript" src="../../_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../../index.html">
          

          
            
            <img src="../../_static/white_logo.png" class="logo" alt="Logo"/>
          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Contents:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../README.html">Product Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../README.html#nauta-user-guide-purpose">Nauta User Guide Purpose</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../installation-and-configuration/README.html">Nauta Installation, Configuration, and Administration Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../installation-and-configuration/README.html#nauta-hardware-requirement-overview">Nauta Hardware Requirement Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../installation-and-configuration/README.html#nauta-installation-procedures">Nauta Installation Procedures</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../installation-and-configuration/README.html#document-flow">Document Flow</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">Nauta</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../index.html">Docs</a> &raquo;</li>
        
      <li>OpenVINO Model Server Overview</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../../_sources/user-guide/actions/openvino_inf.md.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="openvino-model-server-overview">
<h1>OpenVINO Model Server Overview<a class="headerlink" href="#openvino-model-server-overview" title="Permalink to this headline">¶</a></h1>
<p>The OpenVino Model Server (OVMS) is an OpenVINO serving component intended to provide hosting for the OpenVINO inference runtime.</p>
<p>OVMS is an external API that is fully compatible with TF Serving providing an alternative prediction solution for Nauta users. OpenVINO provides one of the most performant inference solutions available on Intel platforms. In many cases (especially in small batch scenarios) it outperforms other inference engines including TF Serving.</p>
<p>OVMS does, however have a limited number of supported topologies and therefore cannot be used as the sole inference runtime on Nauta.  It will be employed as an option for models that meet OpenVINO requirements.</p>
<p>Refer to the <a class="reference external" href="https://www.intel.ai/openvino-model-server-boosts-ai-inference-operations/#gs.zb9emx">OpenVINO White Paper</a> for more information.</p>
</div>
<div class="section" id="inference-on-models-served-by-the-openvino-model-server">
<h1>Inference on Models Served by the OpenVINO Model Server<a class="headerlink" href="#inference-on-models-served-by-the-openvino-model-server" title="Permalink to this headline">¶</a></h1>
<p>To perform a batch or stream inference with OVMS, a model in OpenVINO format is required.
To obtain an MNIST model converted to OVMS format (shown in this example), <strong>refer to <a class="reference internal" href="model_export.html"><span class="doc">exporting models</span></a> for complete instructions.</strong></p>
<p>This section discusses the following topics:</p>
<ul class="simple">
<li><p><a class="reference external" href="#mount-the-input-directory-and-copy-ovms-compatible-model">Mount the Input Directory and Copy OVMS Compatible Model</a></p></li>
<li><p><a class="reference external" href="#models-structure-in-the-input-directory">Models Structure in the Input Directory </a></p></li>
<li><p><a class="reference external" href="#stream-inference">Stream Inference</a></p></li>
<li><p><a class="reference external" href="#batch-prediction">Batch Prediction</a></p></li>
<li><p><a class="reference external" href="#ovms-prediction-with-local-model">OVMS Prediction with Local Model</a></p></li>
</ul>
<div class="section" id="mount-the-input-directory-and-copy-ovms-compatible-model">
<h2>Mount the Input Directory and Copy OVMS Compatible Model<a class="headerlink" href="#mount-the-input-directory-and-copy-ovms-compatible-model" title="Permalink to this headline">¶</a></h2>
<ol class="simple">
<li><p>Mount the Nauta input directory via NFS using the following <a class="reference internal" href="mount_exp_input.html"><span class="doc">mount input</span></a> instructions.</p></li>
<li><p>Copy the OVMS compatible model to the input directory.</p></li>
</ol>
</div>
<div class="section" id="models-structure-in-the-input-directory">
<h2>Models Structure in the Input Directory<a class="headerlink" href="#models-structure-in-the-input-directory" title="Permalink to this headline">¶</a></h2>
<p>Place and mount the Models in a directory structure, as depicted in the example below.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>models/
├── model1
│   ├── 1
│   │   ├── ir_model.bin
│   │   └── ir_model.xml
│   └── 2
│       ├── ir_model.bin
│       └── ir_model.xml
└── model2
    └── 1
        ├── ir_model.bin
        ├── ir_model.xml
        └── mapping_config.json
</pre></div>
</div>
<p>In case of MNIST model conversion with <code class="docutils literal notranslate"><span class="pre">model</span> <span class="pre">export</span></code> command, a directory storing one version of the model is created. Due to prediction prerequisite, the model directory structure <em>must</em> meet the following structure requirements:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>models/
└── &lt;directory from model export output&gt;
    └── 1
        ├── saved_model.bin
        ├── saved_model.mapping
        └── saved_model.xml
</pre></div>
</div>
</div>
<div class="section" id="stream-inference">
<h2>Stream Inference<a class="headerlink" href="#stream-inference" title="Permalink to this headline">¶</a></h2>
<p>When the correct model structure is prepared, run model server instance with:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">nctl</span> <span class="n">predict</span> <span class="n">launch</span> <span class="o">-</span><span class="n">n</span> <span class="n">ovmsexample</span> <span class="o">--</span><span class="n">runtime</span> <span class="n">ovms</span> <span class="o">--</span><span class="n">model</span><span class="o">-</span><span class="n">location</span> <span class="o">/</span><span class="n">mnt</span><span class="o">/</span><span class="nb">input</span><span class="o">/</span><span class="n">home</span><span class="o">/</span><span class="n">models</span><span class="o">/</span><span class="n">mnist</span>
</pre></div>
</div>
<p>When <code class="docutils literal notranslate"><span class="pre">nctl</span> <span class="pre">predict</span> <span class="pre">list</span></code> reports the prediction as running (scroll to the right to see the full details):</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">|</span> <span class="n">Prediction</span> <span class="n">instance</span>   <span class="o">|</span> <span class="n">Parameters</span>   <span class="o">|</span> <span class="n">Submission</span> <span class="n">date</span>        <span class="o">|</span> <span class="n">Start</span> <span class="n">date</span>             <span class="o">|</span> <span class="n">Duration</span>      <span class="o">|</span> <span class="n">Owner</span>    <span class="o">|</span> <span class="n">Status</span>   <span class="o">|</span> <span class="n">Template</span> <span class="n">name</span>             <span class="o">|</span> <span class="n">Template</span> <span class="n">version</span>   <span class="o">|</span>
<span class="o">|-----------------------+--------------+------------------------+------------------------+---------------+----------+----------+---------------------------+--------------------|</span>
<span class="o">|</span> <span class="n">ovmsexample</span>           <span class="o">|</span>              <span class="o">|</span> <span class="mi">2019</span><span class="o">-</span><span class="mi">07</span><span class="o">-</span><span class="mi">29</span> <span class="mi">03</span><span class="p">:</span><span class="mi">01</span><span class="p">:</span><span class="mi">08</span> <span class="n">PM</span> <span class="o">|</span> <span class="mi">2019</span><span class="o">-</span><span class="mi">07</span><span class="o">-</span><span class="mi">29</span> <span class="mi">03</span><span class="p">:</span><span class="mi">01</span><span class="p">:</span><span class="mi">12</span> <span class="n">PM</span> <span class="o">|</span> <span class="mi">0</span><span class="n">d</span> <span class="mi">0</span><span class="n">h</span> <span class="mi">26</span><span class="n">m</span> <span class="mi">13</span><span class="n">s</span> <span class="o">|</span> <span class="n">user1</span>    <span class="o">|</span> <span class="n">RUNNING</span>  <span class="o">|</span> <span class="n">openvino</span><span class="o">-</span><span class="n">inference</span><span class="o">-</span><span class="n">stream</span> <span class="o">|</span> <span class="mf">0.1</span><span class="o">.</span><span class="mi">0</span>              <span class="o">|</span>
</pre></div>
</div>
<p>Perform stream inference by executing the following command:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">nctl</span> <span class="n">predict</span> <span class="n">stream</span> <span class="o">--</span><span class="n">name</span> <span class="n">ovmsexample</span> <span class="o">--</span><span class="n">data</span> <span class="nb">input</span><span class="o">.</span><span class="n">json</span>
</pre></div>
</div>
<p>An example content of an <code class="docutils literal notranslate"><span class="pre">input.json</span></code> file can be found in examples of nctl located in:</p>
<p><code class="docutils literal notranslate"><span class="pre">&lt;nctl_directory&gt;/examples/ovms_inference</span></code>.</p>
<p>For <code class="docutils literal notranslate"><span class="pre">input.json</span></code> file delivered in the example result of the stream inference, it will be:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">{</span><span class="s2">&quot;predictions&quot;</span><span class="p">:</span> <span class="p">[[</span><span class="mf">0.0006329981843009591</span><span class="p">,</span> <span class="mf">1.111995175051561e-06</span><span class="p">,</span> <span class="mf">0.00018445802561473101</span><span class="p">,</span> <span class="mf">0.08759918063879013</span><span class="p">,</span> <span class="mf">1.9286260055650928e-07</span><span class="p">,</span> <span class="mf">0.9085237383842468</span><span class="p">,</span> <span class="mf">2.53505368164042e-05</span><span class="p">,</span> <span class="mf">0.0012352498015388846</span><span class="p">,</span> <span class="mf">0.0017150170169770718</span><span class="p">,</span> <span class="mf">8.265616634162143e-05</span><span class="p">]]}</span>
</pre></div>
</div>
<p>The output of the prediction, in case of MNIST digit recognition model is a vector of 10 elements. This is the <em>Index</em>: the vector that has highest value, and represents predicted class. In this case, the highest value was reported at <em>Index 5</em>, which corresponds to class of ‘five’ digits.</p>
<p><strong>Note:</strong> Similar JSON files can be generated with python script in: <code class="docutils literal notranslate"><span class="pre">&lt;nctl_directory&gt;/examples/ovms_inference</span></code>, as shown in the example below.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">cd</span> <span class="o">&lt;</span><span class="n">nctl_directory</span><span class="o">&gt;/</span><span class="n">examples</span><span class="o">/</span><span class="n">ovms_inference</span>
<span class="n">python3</span> <span class="o">-</span><span class="n">m</span> <span class="n">venv</span> <span class="o">.</span><span class="n">venv</span>
<span class="n">source</span> <span class="o">.</span><span class="n">venv</span><span class="o">/</span><span class="nb">bin</span><span class="o">/</span><span class="n">activate</span>
<span class="n">pip</span> <span class="n">install</span> <span class="o">-</span><span class="n">r</span> <span class="n">requirements</span><span class="o">.</span><span class="n">txt</span>
</pre></div>
</div>
<p>When venv is prepared and invoked, it appears as:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">python</span> <span class="n">generate_json</span><span class="o">.</span><span class="n">py</span> <span class="o">--</span><span class="n">image_id</span> <span class="o">&lt;</span><span class="n">IMAGE</span> <span class="n">ID</span><span class="o">&gt;</span>
</pre></div>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">IMAGE</span> <span class="pre">ID</span></code> argument determines with s picture from MNIST what will be used.</p>
</div>
<div class="section" id="batch-prediction">
<h2>Batch Prediction<a class="headerlink" href="#batch-prediction" title="Permalink to this headline">¶</a></h2>
<p>To perform batch prediction, generate the correct protobuffers for inference. This step is similar to <a class="reference external" href="batch_inf_example.md#mnist-data-preprocessing">MNIST Data Preprocessing</a>, but with one difference. During the model conversion to an OV format, some of the model signatures information is missing. To perform prediction on
the converted MNIST model, one additional parameter has to be used with <code class="docutils literal notranslate"><span class="pre">mnist_converter_pb.py</span></code>:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">mnist_converter_pb</span><span class="o">.</span><span class="n">py</span> <span class="o">--</span><span class="n">model_input_name</span><span class="o">=</span><span class="s2">&quot;x/placeholder_port_0&quot;</span>
</pre></div>
</div>
<p>After file generation, move the directory that contains the <code class="docutils literal notranslate"><span class="pre">.pb</span></code> files to the <code class="docutils literal notranslate"><span class="pre">/input</span></code> mount point.</p>
<p>When all files are prepared, schedule a prediction with the following command:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">nctl</span> <span class="n">predict</span> <span class="n">batch</span> <span class="o">-</span><span class="n">n</span> <span class="n">ovmsbatch</span> <span class="o">-</span><span class="n">rt</span> <span class="n">ovms</span> <span class="o">--</span><span class="n">model</span><span class="o">-</span><span class="n">location</span> <span class="o">/</span><span class="n">mnt</span><span class="o">/</span><span class="nb">input</span><span class="o">/</span><span class="n">home</span><span class="o">/</span><span class="n">models</span><span class="o">/</span><span class="n">mnist</span> <span class="o">--</span><span class="n">data</span> <span class="o">/</span><span class="n">mnt</span><span class="o">/</span><span class="nb">input</span><span class="o">/</span><span class="n">home</span><span class="o">/</span><span class="n">ovms_inference</span>
</pre></div>
</div>
<p><strong>Note:</strong> The above command assumes that<code class="docutils literal notranslate"><span class="pre">.pb</span></code> files are stored in the <code class="docutils literal notranslate"><span class="pre">ovms_inference</span></code> directory in the <code class="docutils literal notranslate"><span class="pre">/input</span></code> shared folder.</p>
<p>When a batch prediction reaches the <code class="docutils literal notranslate"><span class="pre">FINISHED</span></code> state (as shown in the example), it displays the results (scroll to the right to see the full details). =======</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">|</span> <span class="n">Prediction</span> <span class="n">instance</span>         <span class="o">|</span> <span class="n">Parameters</span>   <span class="o">|</span> <span class="n">Submission</span> <span class="n">date</span>        <span class="o">|</span> <span class="n">Start</span> <span class="n">date</span>             <span class="o">|</span> <span class="n">Duration</span>     <span class="o">|</span> <span class="n">Owner</span>    <span class="o">|</span> <span class="n">Status</span>   <span class="o">|</span> <span class="n">Template</span> <span class="n">name</span>            <span class="o">|</span> <span class="n">Template</span> <span class="n">version</span>   <span class="o">|</span>
<span class="o">|-----------------------------+--------------+------------------------+------------------------+--------------+----------+----------+--------------------------+--------------------|</span>
<span class="o">|</span> <span class="n">mnist</span><span class="o">-</span><span class="mi">526</span><span class="o">-</span><span class="mi">19</span><span class="o">-</span><span class="mi">07</span><span class="o">-</span><span class="mi">30</span><span class="o">-</span><span class="mi">00</span><span class="o">-</span><span class="mi">34</span><span class="o">-</span><span class="mi">07</span> <span class="o">|</span>              <span class="o">|</span> <span class="mi">2019</span><span class="o">-</span><span class="mi">07</span><span class="o">-</span><span class="mi">30</span> <span class="mi">12</span><span class="p">:</span><span class="mi">34</span><span class="p">:</span><span class="mi">35</span> <span class="n">AM</span> <span class="o">|</span> <span class="mi">2019</span><span class="o">-</span><span class="mi">07</span><span class="o">-</span><span class="mi">30</span> <span class="mi">12</span><span class="p">:</span><span class="mi">34</span><span class="p">:</span><span class="mi">45</span> <span class="n">AM</span> <span class="o">|</span> <span class="mi">0</span><span class="n">d</span> <span class="mi">0</span><span class="n">h</span> <span class="mi">0</span><span class="n">m</span> <span class="mi">17</span><span class="n">s</span> <span class="o">|</span> <span class="n">user1</span>   <span class="o">|</span> <span class="n">COMPLETE</span> <span class="o">|</span> <span class="n">openvino</span><span class="o">-</span><span class="n">inference</span><span class="o">-</span><span class="n">batch</span> <span class="o">|</span> <span class="mf">0.1</span><span class="o">.</span><span class="mi">0</span>              <span class="o">|</span>
</pre></div>
</div>
<p>To understand these results of the <code class="docutils literal notranslate"><span class="pre">/output</span></code> mount point, refer to <a class="reference external" href="working_with_datasets.md#mounting-and-accessing-folders">Working with Datasets</a>.</p>
</div>
<div class="section" id="ovms-prediction-with-local-model">
<h2>OVMS Prediction with Local Model<a class="headerlink" href="#ovms-prediction-with-local-model" title="Permalink to this headline">¶</a></h2>
<p>Nauta platform models can also be forwarded without the <code class="docutils literal notranslate"><span class="pre">/input</span></code> mount. This can be performed with the <code class="docutils literal notranslate"><span class="pre">--local-model-location</span></code> option.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">nctl</span> <span class="n">predict</span> <span class="n">launch</span> <span class="o">-</span><span class="n">n</span> <span class="n">localovms</span> <span class="o">--</span><span class="n">runtime</span> <span class="n">ovms</span> <span class="o">--</span><span class="n">local</span><span class="o">-</span><span class="n">model</span><span class="o">-</span><span class="n">location</span> <span class="o">/</span><span class="n">tmp</span><span class="o">/</span><span class="n">models</span><span class="o">/</span><span class="n">mnist</span><span class="o">/</span>
</pre></div>
</div>
<p><strong>Note:</strong> The above command assumes the MNIST in the OV format is stored in the <code class="docutils literal notranslate"><span class="pre">/tmp/models/mnist</span></code> folder.</p>
</div>
<hr class="docutils" />
<div class="section" id="return-to-start-of-document">
<h2>Return to Start of Document<a class="headerlink" href="#return-to-start-of-document" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p><a class="reference internal" href="../README.html"><span class="doc">README</span></a></p></li>
</ul>
<hr class="docutils" />
</div>
</div>


           </div>
           
          </div>
          <footer>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2019, Intel Corporation

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>